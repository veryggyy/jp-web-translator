import streamlit as st
import requests
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
import time
import random

# 1. é é¢é…ç½®ï¼šç¶­æŒå¤œé–“ç´”æ·¨æ¨¡å¼
st.set_page_config(page_title="å°èªªè­¯é–± Pro - é˜²å°é–ç‰ˆ", page_icon="ğŸŒ™", layout="centered")

# 2. CSSï¼šå¤œé–“æ²‰æµ¸å¼æ’ç‰ˆ
st.markdown("""
    <style>
    .stApp { background-color: #0F0F0F; color: #E0E0E0; } 
    .novel-container {
        max-width: 850px;
        margin: 20px auto;
        padding: 50px 40px;
        background-color: #1A1A1A;
        border: 1px solid #333333;
        border-radius: 16px;
        box-shadow: 0 15px 35px rgba(0,0,0,0.6);
    }
    .novel-title {
        font-family: "Noto Serif TC", serif;
        color: #FFFFFF;
        text-align: center;
        border-bottom: 2px solid #2D2D2D;
        padding-bottom: 30px;
        margin-bottom: 40px;
        font-size: 2.2rem;
    }
    .paragraph-block { margin-bottom: 30px; line-height: 2.0; }
    .zh-content { font-size: 1.25rem; color: #D6D6D6; text-indent: 2.5em; font-family: "Microsoft JhengHei", sans-serif; }
    .jp-orig { display: block; font-size: 0.9rem; color: #666666; margin-top: 8px; border-left: 3px solid #4A90E2; padding-left: 15px; font-style: italic; }
    #MainMenu, footer { visibility: hidden; }
    </style>
    """, unsafe_allow_html=True)

# 3. ç¿»è­¯æ ¸å¿ƒï¼šç©ºç™½é ç•™èˆ‡æ‰¹æ¬¡è™•ç†
def batch_translate_safe(text_list):
    if not text_list: return []
    
    # è´…è©é»‘åå–®
    blacklist = ['ä¸‹ä¸€é ', 'ä¸‹ä¸€ä¸€å€‹', 'å‰ä¸€é ', 'æ¬¡ã¸', 'å‰ã¸', 'ç›®æ¬¡', 'åŠ å…¥æ›¸ç±¤']
    
    processed_list = []
    to_translate = []
    
    for t in text_list:
        clean_t = t.strip()
        # é‚è¼¯ï¼šç©ºç™½æˆ–é»‘åå–®æ–‡å­—ï¼Œæ¨™è¨˜ç‚ºè·³é
        if not clean_t or any(noise in clean_t for noise in blacklist):
            processed_list.append("__EMPTY_LINE__")
        else:
            processed_list.append(clean_t)
            to_translate.append(clean_t)
            
    if not to_translate:
        return [None for _ in processed_list]

    try:
        combined = "\n\n###\n\n".join(to_translate)
        translated_all = GoogleTranslator(source='ja', target='zh-TW').translate(combined)
        translated_parts = translated_all.split("\n\n###\n\n")
    except:
        translated_parts = [GoogleTranslator(source='ja', target='zh-TW').translate(t) for t in to_translate]

    final_results = []
    ti = 0
    for item in processed_list:
        if item == "__EMPTY_LINE__":
            final_results.append(None)
        else:
            final_results.append(translated_parts[ti] if ti < len(translated_parts) else item)
            ti += 1
    return final_results

# 4. ä¸»ç¨‹å¼ä»‹é¢
st.markdown('<h1 style="text-align:center; color:#4A90E2;">ğŸŒ™ å°èªªè­¯é–±ï½œç´”æ·¨å¤œé–“æ¨¡å¼</h1>', unsafe_allow_html=True)
url = st.text_input("è«‹è²¼ä¸Šæ—¥æ–‡å°èªªç¶²å€ï¼š", placeholder="https://ncode.syosetu.com...")

if url:
    try:
        with st.spinner("ğŸŒ™ æ­£åœ¨å˜—è©¦ç¹éä¼ºæœå™¨æª¢æ¸¬..."):
            # æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨çš„ Header çµ„åˆ
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ja,zh-TW;q=0.9,zh;q=0.8',
                'Referer': 'https://ncode.syosetu.com',
                'Cache-Control': 'max-age=0',
                'Connection': 'keep-alive'
            }
            
            # åŠ å…¥éš¨æ©Ÿå»¶é² 1-2 ç§’ï¼Œé˜²æ­¢è¢«ç§’å°
            time.sleep(random.uniform(1, 2))
            
            # ä½¿ç”¨ Session ä¿æŒé€£ç·šç‹€æ…‹
            session = requests.Session()
            res = session.get(url, headers=headers, timeout=20)
            res.raise_for_status()
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'lxml')

            # å®šä½æœ¬æ–‡
            main_content = soup.select_one('#novel_honbun, .novel_view, .episode-content')
            if not main_content: main_content = soup

            # æ¨™é¡Œç¿»è­¯
            raw_title = soup.title.string.split('ã€Œ')[-1].split('ã€')[0] if soup.title else "ç« ç¯€å…§å®¹"
            zh_title = GoogleTranslator(source='ja', target='zh-TW').translate(raw_title)

            st.markdown(f'<div class="novel-container"><h2 class="novel-title">{zh_title}</h2>', unsafe_allow_html=True)

            # æŠ“å–æ®µè½
            paragraphs = [p.get_text() for p in main_content.find_all(['p', 'h1', 'h2'])]
            
            batch_size = 15
            for i in range(0, len(paragraphs), batch_size):
                batch = paragraphs[i:i+batch_size]
                translated_batch = batch_translate_safe(batch)
                
                for orig, tran in zip(batch, translated_batch):
                    if tran is None:
                        st.markdown('<br>', unsafe_allow_html=True)
                    else:
                        st.markdown(f"""
                            <div class="paragraph-block">
                                <div class="zh-content">{tran}</div>
                                <div class="jp-orig">{orig}</div>
                            </div>
                        """, unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
            st.toast("âœ… å…§å®¹æŠ“å–æˆåŠŸä¸¦å·²ç¿»è­¯å®Œæˆã€‚")

    except requests.exceptions.HTTPError as e:
        if "403" in str(e):
            st.error("âŒ å­˜å–é­æ‹’ (403)ã€‚Syosetu æš«æ™‚å°é–äº†é€£ç·šã€‚")
            st.info("è«‹å˜—è©¦æ›´æ› IP (ä½¿ç”¨ VPN) æˆ–æ˜¯éå¹¾åˆ†é˜å¾Œå†é‡æ–°åŸ·è¡Œã€‚")
        else:
            st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    except Exception as e:
        st.error(f"âŒ ç³»çµ±éŒ¯èª¤ï¼š{str(e)}")
